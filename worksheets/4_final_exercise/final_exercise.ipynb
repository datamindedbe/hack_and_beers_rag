{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40d93e67",
   "metadata": {},
   "source": [
    "# Final Exercise\n",
    "\n",
    "This exercise is about bringing together what you have learnt in the previous exercise\n",
    "\n",
    "In the source_documents folder are a number of CV's of fictional people.\n",
    "\n",
    "The first task would be to embed this data into a vector database, secondly create a retrieval and generation function which will make use of this data.\n",
    "\n",
    "You can make the RAG step as simple or as complex as you like - consider some of the following questions:\n",
    "\n",
    "* how best to chunk the data?\n",
    "* what information to pass to the generation step? is any preprocessing/augmentation needed\n",
    "* are there any metrics you can incorporate to your pipeline at runtime? (NOTE: avoid using recall as this can become computationally expensive)\n",
    "* what possible data posioning attacks might be relevent to this exercise? How, can you protect against them?\n",
    "* ... be creative and think of other improvements you may like to implement\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbc7d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initial setup\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import openai\n",
    "import glob\n",
    "import dotenv\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "from src.utils import OPENAI_API_KEY\n",
    "from src.chroma_db import VectorCollection, VectorDBItem, OpenAIEmbeddingModel, get_chromadb_client, remove_collection\n",
    "\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "OPENAI_MODEL = 'gpt-4-turbo' # 128,000 tokens\n",
    "SCHEMA_NAME = \"final_exercise_embeddings\"\n",
    "COLLECTION_NAME = \"final_exercise_collection\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee81af5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_client = get_chromadb_client(SCHEMA_NAME)\n",
    "client_openai = openai.OpenAI(\n",
    "    api_key=OPENAI_API_KEY\n",
    ")\n",
    "\n",
    "collection = VectorCollection(\n",
    "    name=COLLECTION_NAME,\n",
    "    client=chroma_client, \n",
    "    token=OPENAI_API_KEY, #type: ignore,\n",
    "    embedding_model=OpenAIEmbeddingModel.ADA_002 # you can try other available models as well\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2382e953",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents():\n",
    "    \"\"\" A very simple method to obtain all documents as a list\"\"\"\n",
    "    documents = []\n",
    "    for file_path in glob.glob(\"source_documents/*.md\"):\n",
    "        with open(file_path, 'r') as f:\n",
    "            content = f.read()\n",
    "            doc_id = file_path.split('/')[-1].replace('.md', '')\n",
    "            documents.append((doc_id, content))\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394b0924",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Try to implement one of the chunking methods to improve the performance of your RAG! You can include this method\n",
    "\"\"\"\n",
    "def get_chunks(document):\n",
    "    # Simple whitespace-based chunking\n",
    "    return document.split(\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70ab559",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load documents (in full or chunked) into the vector database\"\"\"\n",
    "\n",
    "documents = load_documents()\n",
    "for doc_id, content in documents:\n",
    "    \n",
    "    # For chunking implementation, uncomment below and comment the add_item line\n",
    "    # document_chunks = get_chunks(content)\n",
    "    # for i, chunk in enumerate(document_chunks):\n",
    "    #     chunk_id = f\"{doc_id}_chunk_{i}\"\n",
    "    #     collection.add_item(chunk, chunk_id)\n",
    "\n",
    "    collection.add_item(content, doc_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63ae2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(query, top_k):\n",
    "    \"\"\" A method to retrieve topK results similar to the given query \"\"\"\n",
    "\n",
    "    results = collection.similar_items(query, n_results=top_k)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    return [result.text for result in results]\n",
    "\n",
    "\n",
    "\n",
    "def generate(query, context, max_tokens=500):\n",
    "    \"\"\" Wrapper on the OpenAI generation method, which combines the retrieved context together with the user query \"\"\"\n",
    "\n",
    "    prompt = f\"Context: {context}\\n\\nQuestion: {query}\\n\\nAnswer:\"\n",
    "    response = client_openai.chat.completions.create(\n",
    "        model=OPENAI_MODEL,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=max_tokens\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "def rag(query, top_k=3):\n",
    "    \"\"\" The entire RAG pipeline (Retrieve from VectorDB -> Generate) \"\"\"\n",
    "    \n",
    "    retrieved_docs = retrieve(query, top_k)\n",
    "    context = \"\\n\\n\".join(retrieved_docs)\n",
    "\n",
    "    return generate(query, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5289dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = rag(\"Which candidate should I hire for a senior Python developer role?\")\n",
    "\n",
    "display(Markdown(answer))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hack-and-beers-rag-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
