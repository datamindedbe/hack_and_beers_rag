{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81ff8f95",
   "metadata": {},
   "source": [
    "# RAGAS Framework\n",
    "We will be making use of the RAGAS (RAG Assessement) framework to explore the concept of \n",
    "LLM Judge based metrics.\n",
    "for more information about this framework go to https://docs.ragas.io/en/v0.3.0/getstarted/ (information in this worksheet is taken from there)\n",
    "\n",
    "The first thing we will be looking into are some of the available metrics you can use to evaluate your RAG system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9b3d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = #TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e855f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the first which we need to do is link the llm we want to use to the RAGAS framework\n",
    "\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from langchain_openai import ChatOpenAI\n",
    "import openai\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o\"))\n",
    "openai_client = openai.OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d133dd51",
   "metadata": {},
   "source": [
    "## Context Precision\n",
    "\n",
    "Context Precision is a metric that evaluates the retriever's ability to rank relevant chunks higher than irrelevant ones for a given query in the retrieved context. Specifically, it assesses the degree to which relevant chunks in the retrieved context are placed at the top of the ranking.\n",
    "\n",
    "It is calculated as the mean of the precision@k for each chunk in the context. Precision@k is the ratio of the number of relevant chunks at rank k to the total number of chunks at rank k.\n",
    "\n",
    "\n",
    "<div>\n",
    "<img src=\"context_precision_formula.png\" width=\"700\"/>\n",
    "</div>\n",
    "\n",
    "The reason why we care about the positioning in this metric is that items closer to the front of the context window will be the given the \"most importance\" by the LLM. for more information on this see https://arxiv.org/abs/2307.03172"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b26ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: run this cell, play around with the inputs to understand how the metric is affected\n",
    "# add more retrieved contexts (both relevant and irrelevant) to see how the score changes\n",
    "\n",
    "from ragas import SingleTurnSample\n",
    "from ragas.metrics import LLMContextPrecisionWithoutReference\n",
    "\n",
    "context_precision = LLMContextPrecisionWithoutReference(llm=evaluator_llm)\n",
    "\n",
    "sample = SingleTurnSample(\n",
    "    user_input=\"Where is the Eiffel Tower located?\",\n",
    "    response=\"\",\n",
    "    retrieved_contexts=[\"The Statue of Liberty is in New York.\",\"The Eiffel Tower is located in Paris.\",], \n",
    ")\n",
    "\n",
    "\n",
    "await context_precision.single_turn_ascore(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ad1bd7",
   "metadata": {},
   "source": [
    "## Context Recall\n",
    "\n",
    "Context Recall measures how many of the relevant documents (or pieces of information) were successfully retrieved. It focuses on not missing important results. Higher recall means fewer relevant documents were left out.\n",
    "\n",
    "\n",
    "<div>\n",
    "<img src=\"context_recall_formula.png\" width=\"700\"/>\n",
    "</div>\n",
    "\n",
    "LLM based recall calculations are very expensive as they require evaluating if each piece of the embedded data is relevant to the question at hand. \n",
    "\n",
    "\n",
    "Within RAGAS the approach taken to compute recall is slightly confusing, but essentially the source information is passed to the metric calculator (reference argument), this is then broken up into a number of claims, recall is calculated based on the number of relevant claims in the source data and the number of relevant claims in the retrieved context\n",
    "\n",
    "<div>\n",
    "<img src=\"context_recall_ragas_formula.png\" width=\"700\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af75e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: run this cell, play around with the inputs to understand how the metric is affected\n",
    "\n",
    "from ragas.dataset_schema import SingleTurnSample\n",
    "from ragas.metrics import LLMContextRecall\n",
    "\n",
    "sample = SingleTurnSample(\n",
    "    user_input=\"where is the barno tower located?\",\n",
    "    response=\"\",\n",
    "    reference=\"The barno tower is located in Brussels. the statue of liberty is in new york, barno tower is not very far from the grand place\",\n",
    "    retrieved_contexts=[\"brussels is home to the barno tower\"], \n",
    ")\n",
    "\n",
    "context_recall = LLMContextRecall(llm=evaluator_llm)\n",
    "await context_recall.single_turn_ascore(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe39c1a8",
   "metadata": {},
   "source": [
    "## Response Relevancy\n",
    "The ResponseRelevancy metric measures how relevant a response is to the user input. Higher scores indicate better alignment with the user input, while lower scores are given if the response is incomplete or includes redundant information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16dd3d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: run this cell, play around with the inputs to understand how the metric is affected\n",
    "from ragas import SingleTurnSample \n",
    "from ragas.metrics import ResponseRelevancy\n",
    "\n",
    "sample = SingleTurnSample(\n",
    "        user_input=\"where is paris\",\n",
    "        response=\"paris is in france\",\n",
    "        retrieved_contexts=[]\n",
    "    )\n",
    "\n",
    "scorer = ResponseRelevancy(llm=evaluator_llm, embeddings=embeddings)\n",
    "await scorer.single_turn_ascore(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45db25ac",
   "metadata": {},
   "source": [
    "## Faithfullness\n",
    "\n",
    "The Faithfulness metric measures how factually consistent a response is with the retrieved context. It ranges from 0 to 1, with higher scores indicating better consistency.\n",
    "\n",
    "<div>\n",
    "<img src=\"faithfullness_formula.png\" width=\"700\"/>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c4abef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: run this cell, play around with the inputs to understand how the metric is affected\n",
    "\n",
    "from ragas.dataset_schema import SingleTurnSample \n",
    "from ragas.metrics import Faithfulness\n",
    "\n",
    "sample = SingleTurnSample(\n",
    "        user_input=\"\",\n",
    "        response=\"brussels is the capital of belgium\",\n",
    "        retrieved_contexts=[\n",
    "            \"capital cities of europe include paris, berlin, madrid, rome and brussels. brussels is in belgium\",\n",
    "        ]\n",
    "    )\n",
    "scorer = Faithfulness(llm=evaluator_llm)\n",
    "await scorer.single_turn_ascore(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21753c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Optional (simple) explore additional assessment metrics available in RAGAS \n",
    "# see https://docs.ragas.io/en/v0.3.0/concepts/metrics/available_metrics/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vaia_rag_workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
