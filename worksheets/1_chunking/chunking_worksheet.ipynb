{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0463df1",
   "metadata": {},
   "source": [
    "# Introduction to Chunking\n",
    "In this exercise we we focus on  chunking - which is the process of splitting up documents into smaller pieces \n",
    "which can then be retrieved during the RAG process.\n",
    "This worksheet starts with simple chunking approches and then shows more advanced techniques. however, it is worth noting\n",
    "that more advanced does not always mean better - there are always tradeoffs to be made regarding performance, complexity and cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9d4f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initial setup\n",
    "import sys\n",
    "import os\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "from src.utils import OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087ba6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "for this exercise we will use a text document - \n",
    "and apply different chunking techniques to it.\n",
    "\"\"\"\n",
    "\n",
    "# call open ai with the chunking example\n",
    "import openai\n",
    "from typing import Tuple\n",
    "from src.openai import get_response\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "\n",
    "def load_example_text()->str:\n",
    "    with open(\"example_document.txt\", \"r\") as file:\n",
    "        txt = file.read()\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0a12f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "fixed length chunking:\n",
    "the most basic approach to chunking - take your document and split\n",
    "it into equal sized pieces (either by character count, token count or word count)\n",
    "\n",
    "the advantage of fixed length chunking is that it is simple to implement and run\n",
    "however the disadvantage is that it does not do a good job of preserving context, there \n",
    "is always a risk that an idea will be split between multiple chunks which in the \n",
    "worst case will make meaningless as a retrievable element.\n",
    "\n",
    "TODO: run the cell and get a sense of of cases the issues that may arise from fixed \n",
    "length chunking. play around with chunk size to see how it can impact the preservation/\n",
    "destruction of context\n",
    "\"\"\"\n",
    "\n",
    "CHUNK_SIZE_CHARACTERS = 100\n",
    "\n",
    "input_txt = load_example_text()\n",
    "# remove paragraph and link breaks\n",
    "input_txt = input_txt.replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "chunks =  [input_txt[i:i + CHUNK_SIZE_CHARACTERS] for i in range(0, len(input_txt), CHUNK_SIZE_CHARACTERS)]\n",
    "\n",
    "\n",
    "print(f\"split example into {len(chunks)} chunks\")\n",
    "for index,chunk in enumerate(chunks):\n",
    "    print(f\"chunk {index}: {chunk}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5374ca19",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "sliding window chunking:\n",
    "given that fixed length chunking has a tendency to break up context at \"unnatural\" boundaries\n",
    "1 solution to this would be to use an overlap between chunks. the assumption being that even if a piece of context\n",
    "is incomplete in one chunk it will be fully captured in another thanks to the overlap. \n",
    "Implicit in this approach is the fact that increase the amount of data which you then embedded due to the redundancy\n",
    "\n",
    "TODO: run and understand what this code is doing, tweak the chunk size and overlap factor and observe\n",
    "the impact that has on the preservation of context, think about the tradeoff between overlap size and context preservation\n",
    "\"\"\"\n",
    "\n",
    "CHUNK_SIZE_CHARACTERS = 100\n",
    "OVERLAP_SIZE_CHARACTERS = 30\n",
    "\n",
    "input_txt = load_example_text()\n",
    "# remove paragraph and link breaks\n",
    "input_txt = input_txt.replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "\n",
    "chunks = []\n",
    "for i in range(0, len(input_txt), CHUNK_SIZE_CHARACTERS - OVERLAP_SIZE_CHARACTERS):\n",
    "    chunks.append(input_txt[i:i + CHUNK_SIZE_CHARACTERS])\n",
    "\n",
    "print(f\"split example into {len(chunks)} chunks\")\n",
    "for index,chunk in enumerate(chunks):\n",
    "    print(f\"chunk {index}: {chunk}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6133b078",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "semantic chunking: \n",
    "the examples above took a programatic approach to chunking without considering the specifics of the\n",
    "information we want to chunk - given that preserving the semantic information is crucial to the \n",
    "performance of the rag we should also consider approaches which more directly aim to preserve this semantic information\n",
    "\n",
    "document structure guided chunking - a 'obvious' approach we should consider is using the punctuation breaks given\n",
    "in the document itself to form our chunk boundaries.\n",
    "ie -> using sentence breaks & paragraph breaks as a way to do this separation\n",
    "\n",
    "TODO: what would be the pitfalls of this approach? \n",
    "\"\"\"\n",
    "\n",
    "BREAK_CHARACTERS = [\".\", \"!\", \"?\", \"\\n\",\"\\r\"]\n",
    "input_txt = load_example_text()\n",
    "\n",
    "chunks = []\n",
    "current_chunk = \"\"\n",
    "\n",
    "for char in input_txt:\n",
    "    current_chunk += char\n",
    "    if char in BREAK_CHARACTERS:\n",
    "        chunks.append(current_chunk.strip())\n",
    "        current_chunk = \"\"\n",
    "\n",
    "if current_chunk:\n",
    "    chunks.append(current_chunk.strip())\n",
    "\n",
    "# remove empty chunks\n",
    "chunks = [chunk for chunk in chunks if chunk]\n",
    "\n",
    "print(f\"split example into {len(chunks)} chunks\")\n",
    "chunk_sizes = [len(chunk) for chunk in chunks]\n",
    "print(f\"average chunk size: {sum(chunk_sizes) / len(chunk_sizes)}\")\n",
    "print(f\"min chunk size: {min(chunk_sizes)}\")\n",
    "print(f\"max chunk size: {max(chunk_sizes)}\")\n",
    "\n",
    "for index,chunk in enumerate(chunks):\n",
    "    print(f\"chunk {index}: {chunk}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fae898",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "agentic chunking: \n",
    "make use of an llm to do the chunking for you. this approach can work very well for finding natural contextual\n",
    "boundaries between chunks. the downside however, is that it comes with a high cost and will not be suited to\n",
    "large / constantly growing datasets\n",
    "\n",
    "TODO: run the script to do chunking with the help of an LLM, play around with the prompt - to improve its performance\n",
    "\"\"\"\n",
    "\n",
    "client = openai.OpenAI(\n",
    "    api_key=openai.api_key\n",
    ")\n",
    "\n",
    "\n",
    "input_txt = load_example_text()\n",
    "full_response, _ = get_response(client, f\"create smaller chunks of approximatelly 100 characters for the following text but prioritize preserving the semantic context of the chunks, separate the chunks using | characters and : {input_txt}\", model=\"gpt-4\")\n",
    "break_character = \"|\"\n",
    "chunks = [chunk.strip() for chunk in full_response.split(break_character) if chunk.strip()]\n",
    "\n",
    "print(f\"split example into {len(chunks)} chunks\")\n",
    "chunk_sizes = [len(chunk) for chunk in chunks]\n",
    "print(f\"average chunk size: {sum(chunk_sizes) / len(chunk_sizes)}\")\n",
    "print(f\"min chunk size: {min(chunk_sizes)}\")\n",
    "print(f\"max chunk size: {max(chunk_sizes)}\")\n",
    "\n",
    "for index,chunk in enumerate(chunks):\n",
    "    print(f\"chunk {index}: {chunk}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b72da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: read this article on different chunking strategies and there tradeoffs\n",
    "# https://masteringllm.medium.com/11-chunking-strategies-for-rag-simplified-visualized-df0dbec8e373 \n",
    "\n",
    "# TODO (Optional)\n",
    "# implement a chunking approach we have not already covered here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hack-and-beers-rag-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
